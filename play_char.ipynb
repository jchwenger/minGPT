{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 9693248 characters, 289 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2020 01:01:12 - INFO - mingpt.model -   number of parameters: 1.744384e+06\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=2, n_head=2, n_embd=256)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blah\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "fname = 'le_model.pt'\n",
    "if os.path.isfile(fname):\n",
    "    model.load_state_dict(torch.load(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 101: train loss 1.32109. lr 5.999973e-04:   0%|          | 101/37864 [00:10<1:41:47,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "When they have clearned been of the world wrong fire.. At last the depition\n",
      "the play the superfic'd have me more of fair\n",
      "\n",
      "\n",
      "**\n",
      "\n",
      "here was\n",
      "if you not that he was the man?\n",
      "\n",
      "start wall the\n",
      "\n",
      "at least it w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 201: train loss 1.24467. lr 5.999895e-04:   1%|          | 201/37864 [00:19<1:41:00,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "crutal\n",
      "\n",
      "**\n",
      "\n",
      "la refrain rean scorner..re thing. still. \n",
      "\n",
      "in the beautiful, etc..\n",
      "\n",
      "of lacanage, if you could be it is; switness to me, you are if once, a while before, if I\n",
      "    made none be his face o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 301: train loss 1.25692. lr 5.999765e-04:   1%|          | 301/37864 [00:28<1:40:35,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "I don’t know it want anything about it. I could have. Take a matter stick. To do this beauty: evil a tool in this arms. This hours are survived. It’s alg. Yeah. Not ever. Its least you not. It’s obv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 401: train loss 1.25743. lr 5.999583e-04:   1%|          | 401/37864 [00:38<1:40:41,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "Improve a ping\n",
      "Ok\n",
      "\n",
      "**\n",
      "\n",
      "Maybe you\n",
      "Medy\n",
      "Shape\n",
      "The truth\n",
      "Baby\n",
      "Shape\n",
      "Then\n",
      "Correct\n",
      "Exposition\n",
      "A day significant of course\n",
      "The scholar century\n",
      "English in this thing\n",
      "Musing\n",
      "Maybe\n",
      "If that work.\n",
      "\n",
      "*\n",
      "\n",
      "There on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 501: train loss 1.20856. lr 5.999350e-04:   1%|▏         | 502/37864 [00:48<1:39:25,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "As if it well is not with oh yes not anywhere. All have. All obvious. Haha. Haha oh no. Haha. Somehow. Therealisces? Haha. Blah. \n",
      "\n",
      "**\n",
      "\n",
      "the mediorating. something that. fuck. the fucking sing another\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 601: train loss 1.25767. lr 5.999065e-04:   2%|▏         | 602/37864 [00:57<1:41:20,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "Structur's clothes of massive service. Burnstance. Sure. Etc. Stretched. And not like the real the pucking to be. Yes. You know. That’s the fox. That’s the positive, if you don’t remain text any coo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 701: train loss 1.23016. lr 5.998729e-04:   2%|▏         | 702/37864 [01:07<1:58:23,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "The self texts must be alone, the rest, merry, of the whole from time to send him in a day for an except to resuing sanctive things. You might be much so much as much, etc. At the music. Totally the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 801: train loss 1.24239. lr 5.998341e-04:   2%|▏         | 802/37864 [01:17<1:41:04,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "I should have it think here all this after all then both as\n",
      "\n",
      "(Oh yes, old standads you could flesh as well his hand, stole image, thou see, as one hand importancy shatting the creatie of the dugstor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 901: train loss 1.18058. lr 5.997901e-04:   2%|▏         | 902/37864 [01:27<1:55:55,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "despair with\n",
      "despair\n",
      "\n",
      "feeling\n",
      "piece\n",
      "daily wanable\n",
      "leaving\n",
      "\n",
      "**\n",
      "\n",
      "Devolution is throom. \n",
      "Dislike a limbs\n",
      "\n",
      "**\n",
      "\n",
      "The only\n",
      "\n",
      "**\n",
      "\n",
      "I feel asy I remember as the traumatic cell today witness the realm, etc. etc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1001: train loss 1.24773. lr 5.997410e-04:   3%|▎         | 1002/37864 [01:37<1:50:22,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "this only thing this is not it was a burdene\n",
      "\n",
      "this\n",
      "\n",
      "of collection for a stront of india fount\n",
      "\n",
      "our \n",
      "\n",
      "\n",
      "as infernal\n",
      "\n",
      "\n",
      "stress\n",
      "\n",
      "\n",
      "\n",
      "as in palace\n",
      "\n",
      "\n",
      "as sonable in fun \n",
      "\n",
      "**\n",
      "\n",
      "\n",
      "tas indiry teensition\n",
      "\n",
      "there is \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1101: train loss 1.26808. lr 5.996867e-04:   3%|▎         | 1102/37864 [01:46<1:40:34,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "this way\n",
      "\n",
      "**\n",
      "\n",
      "those things as you\n",
      "too diffeature\n",
      "\n",
      "thought\n",
      "\n",
      "that makes it made the thing to the sance of many would be forced to do that would be something\n",
      "\n",
      "that's bren a few\n",
      "then other song\n",
      "\n",
      "then so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1201: train loss 1.24115. lr 5.996273e-04:   3%|▎         | 1202/37864 [01:56<1:43:22,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "it’s\n",
      "\n",
      "so fuck that’s what they all this whatever, I guess. \n",
      "\n",
      "there’s nothingnable to be something of the time I have to have so done\n",
      "\n",
      "thou lie\n",
      "\n",
      "\n",
      "imbaud back I ten correct etc\n",
      "\n",
      "oh yeah a soint in a t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1301: train loss 1.28767. lr 5.995627e-04:   3%|▎         | 1302/37864 [02:06<1:43:04,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "Struction\n",
      "\n",
      "**\n",
      "\n",
      "Creation\n",
      "Storying old those\n",
      "That\n",
      "Slack \n",
      "\n",
      "And leave\n",
      "Self\n",
      "\n",
      "**\n",
      "\n",
      "credictation.\n",
      "\n",
      "**\n",
      "\n",
      "impossible the style. \n",
      "improved its are something like that is sigh so sten who and obey had the total \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1401: train loss 1.29307. lr 5.994929e-04:   4%|▎         | 1402/37864 [02:16<1:39:18,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "THE END\n",
      "\n",
      "**\n",
      "\n",
      "the pain is note\n",
      "\n",
      "**\n",
      "\n",
      "it's as agonymore pity in try to \n",
      "the meta. it’s the cumson we make me the beggar. yeah. I don’t me. no. haha. yeah. no. oh no of course. yeah. sort of trauming th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1501: train loss 1.20964. lr 5.994180e-04:   4%|▍         | 1502/37864 [02:25<1:39:32,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "The transported by denialish thing falls in this further\n",
      "\n",
      "**\n",
      "\n",
      "that could be alone this wise endless shive\n",
      "\n",
      "\n",
      "\n",
      "no matter exten this than a pite tongue, nor soldier oblivion, to ear from a murder than \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1601: train loss 1.26834. lr 5.993380e-04:   4%|▍         | 1602/37864 [02:35<1:39:12,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "am I was in a particular cutie cry. article\n",
      "difficulty. into the thing.\n",
      "that isn’t ever it on what you wanking syntax. I hate it was making it it brevenge, haha, if hm. Isn’t it hm. \n",
      "\n",
      "As if I was no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1701: train loss 1.24914. lr 5.992528e-04:   4%|▍         | 1702/37864 [02:45<1:40:19,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "shit. yeah. fucking. shit. shit. as if. the focus. instangly. oh yeah as something triers served. or self-hard. so inability text. etc. boring. tirelevaning. somewhere. yes imagining. babsent. say. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1801: train loss 1.26310. lr 5.991624e-04:   5%|▍         | 1802/37864 [02:55<1:41:22,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "slow only \n",
      "drown\n",
      "duter\n",
      "\n",
      "look\n",
      "\n",
      "**\n",
      "\n",
      "stupidity of\n",
      "\n",
      "look\n",
      "the one arse\n",
      "\n",
      "**\n",
      "\n",
      "stupidity cross\n",
      "\n",
      "some futuwed preferences\n",
      "\n",
      "of fortunation\n",
      "\n",
      "middle\n",
      "\n",
      "mind in the subscribeling prisophero to take\n",
      "\n",
      "**\n",
      "\n",
      "must\n",
      "\n",
      "the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1901: train loss 1.21276. lr 5.990669e-04:   5%|▌         | 1902/37864 [03:05<1:37:37,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "I’m not won’t exactly actually said. \n",
      "\n",
      "I was well something you’ve got much on and missing.\n",
      "\n",
      "**\n",
      "\n",
      "Infinite abstracts in the other. Suck youthslapbeys such a proper to love.\n",
      "\n",
      "**\n",
      "\n",
      "Hoof the colovel cell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2001: train loss 1.24319. lr 5.989662e-04:   5%|▌         | 2002/37864 [03:14<1:36:48,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "feeling to stuff\n",
      "or my shift,\n",
      "worseting like there tenace or with the crown.\n",
      "    The possibal. Nounce with him, better sward,\n",
      "    Or two untally sin, and through this to the capital on back\n",
      "    and \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2101: train loss 1.21839. lr 5.988604e-04:   6%|▌         | 2102/37864 [03:24<1:36:42,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "sweather acattered, selection, of chaotice, and on all this for now, which, no. No poor nothing. No result. Not realize. To them late. How if I think of myself? Haha. Haha haha I something else. Or \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2201: train loss 1.21543. lr 5.987495e-04:   6%|▌         | 2202/37864 [03:34<1:37:25,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "I wish maybe with nobly\n",
      "\n",
      "(the parallel\n",
      "nowhere where soint\n",
      "or\n",
      "solid\n",
      "somewhat\n",
      "there somewhere you might be and the real pluck the sheer first. As in? I say two impossely bears this blown one, that th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2301: train loss 1.21543. lr 5.986334e-04:   6%|▌         | 2302/37864 [03:43<1:34:26,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "the mouth now the sauciling\n",
      "\n",
      "and the face\n",
      "\n",
      "**\n",
      "\n",
      "I hate of course I don’t see it. You know. You know. Yeah. You don’t without. Nothing. So still. Yeah. It’s. NOT. Yeah. No end to this is this. Haha. N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2401: train loss 1.23882. lr 5.985122e-04:   6%|▋         | 2402/37864 [03:53<1:37:49,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "It’s fucking feeling song. \n",
      "Tools. Too cross. Trop. As if externity. Too earn doom of these are too late\n",
      "\n",
      "**\n",
      "\n",
      "A lucius\n",
      "\n",
      "**\n",
      "\n",
      "Analysis\n",
      "\n",
      "Always the flowers of that to get me anything. \n",
      "\n",
      "That’s the worl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2501: train loss 1.22853. lr 5.983858e-04:   7%|▋         | 2502/37864 [04:03<1:42:19,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "As if the master of the coronation as if yeah if I saw the only this dismay be that was true. Yeah. For now. There there true? I can’t believe that I’m not done to see a milk or similar eloque tonig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2601: train loss 1.20855. lr 5.982543e-04:   7%|▋         | 2602/37864 [04:13<1:37:54,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "A PROLLES\n",
      "\n",
      "CALIBAN\n",
      "ROLLES\n",
      "ONLY\n",
      "PRIS\n",
      "HATIPHOLUS OF ENEBLARET\n",
      "KEETH\n",
      "NERY\n",
      "NO MUST\n",
      "\n",
      "NUST\n",
      "RAVE, EXT\n",
      "NOYRKS\n",
      "HORKE\n",
      "THORD\n",
      "LORE\n",
      "THO\n",
      "NE\n",
      "REAGED\n",
      "KIPta CONICE\n",
      "RESTART\n",
      "AYER\n",
      "IT\n",
      "NELGWALEY\n",
      "STEPINGEBLE\n",
      "LUCE. Not SICI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2642: train loss 1.19859. lr 5.981989e-04:   7%|▋         | 2643/37864 [04:17<57:06, 10.28it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b9e9f04a14dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                       num_workers=4)\n\u001b[1;32m     12\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/dl/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dl/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "bs = 256\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=1, \n",
    "                      batch_size=bs, \n",
    "                      learning_rate=6e-4,\n",
    "                      lr_decay=True, \n",
    "                      warmup_tokens=bs*20, \n",
    "                      final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "pos_emb \t torch.Size([1, 64, 256])\n",
      "tok_emb.weight \t torch.Size([289, 256])\n",
      "blocks.0.ln1.weight \t torch.Size([256])\n",
      "blocks.0.ln1.bias \t torch.Size([256])\n",
      "blocks.0.ln2.weight \t torch.Size([256])\n",
      "blocks.0.ln2.bias \t torch.Size([256])\n",
      "blocks.0.attn.mask \t torch.Size([1, 1, 64, 64])\n",
      "blocks.0.attn.key.weight \t torch.Size([256, 256])\n",
      "blocks.0.attn.key.bias \t torch.Size([256])\n",
      "blocks.0.attn.query.weight \t torch.Size([256, 256])\n",
      "blocks.0.attn.query.bias \t torch.Size([256])\n",
      "blocks.0.attn.value.weight \t torch.Size([256, 256])\n",
      "blocks.0.attn.value.bias \t torch.Size([256])\n",
      "blocks.0.attn.proj.weight \t torch.Size([256, 256])\n",
      "blocks.0.attn.proj.bias \t torch.Size([256])\n",
      "blocks.0.mlp.0.weight \t torch.Size([1024, 256])\n",
      "blocks.0.mlp.0.bias \t torch.Size([1024])\n",
      "blocks.0.mlp.2.weight \t torch.Size([256, 1024])\n",
      "blocks.0.mlp.2.bias \t torch.Size([256])\n",
      "blocks.1.ln1.weight \t torch.Size([256])\n",
      "blocks.1.ln1.bias \t torch.Size([256])\n",
      "blocks.1.ln2.weight \t torch.Size([256])\n",
      "blocks.1.ln2.bias \t torch.Size([256])\n",
      "blocks.1.attn.mask \t torch.Size([1, 1, 64, 64])\n",
      "blocks.1.attn.key.weight \t torch.Size([256, 256])\n",
      "blocks.1.attn.key.bias \t torch.Size([256])\n",
      "blocks.1.attn.query.weight \t torch.Size([256, 256])\n",
      "blocks.1.attn.query.bias \t torch.Size([256])\n",
      "blocks.1.attn.value.weight \t torch.Size([256, 256])\n",
      "blocks.1.attn.value.bias \t torch.Size([256])\n",
      "blocks.1.attn.proj.weight \t torch.Size([256, 256])\n",
      "blocks.1.attn.proj.bias \t torch.Size([256])\n",
      "blocks.1.mlp.0.weight \t torch.Size([1024, 256])\n",
      "blocks.1.mlp.0.bias \t torch.Size([1024])\n",
      "blocks.1.mlp.2.weight \t torch.Size([256, 1024])\n",
      "blocks.1.mlp.2.bias \t torch.Size([256])\n",
      "ln_f.weight \t torch.Size([256])\n",
      "ln_f.bias \t torch.Size([256])\n",
      "head.weight \t torch.Size([289, 256])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'le_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! O, the good night!\n",
      "    Turn my story, who clothes are young\n",
      "    Of this treasures was that set\n",
      "    And, for a hair affair as such another hand;\n",
      "    And the do is his more passive.\n",
      "    Some worse to the dark poor and danger. And all the livery whele’s written reading after. This content etc. This sudden in me. The old thing is this necessary the destroyed this fucked above of changes of the rought and act, and no chances, when the rest, it shall, I was like to him\n",
      "    thou that my cousin? Why. Where non? I am almost see\n",
      "    Hath too long enough my lands.\n",
      "  LUCILIUS. I will show my countil so bear\n",
      "    Th' eyes, so the drac'd a trust were a cliff an end\n",
      "    I weep to be my speech.\n",
      "  SALERIO. Are you not done, all that would thy move of thy brothers,\n",
      "    That shall be a do farewell, man with mountains,\n",
      "    Or loss in the work of thy traitors have was sent until to my love is consummation was free\n",
      "    to command, in true child saint mutate, to see your bed.\n",
      "  Ham. Ay, my lord, touch terms matter of you.\n",
      "  Pedro. You ship your place\n",
      "    By heart will stay the should not be\n",
      "     That I was by th' earl; I will be so.\n",
      "  COSTARD. That it blacks me to blind.\n",
      "  PROSPERO. Why, this is not thou arise to say?\n",
      "  COUNTESS. Orgiving thee afoot well; where I am perfected.\n",
      "  MRS. PAGE. Prithee, six young are well: I. Ah, my sword! Why dost thou, what I thought I was a goodly the must great late,\n",
      "    To melt, and all he whip thou must fleck.\n",
      "     But if you speak with the prouvenges.\n",
      "  PROVOST. Will it you see? It feels not a fail at a potter?\n",
      "  MALVOLIO. I have none to tell me her in, sir.\n",
      "  CASSIUS. If thou do look on't all tried!\n",
      "    Have I not a wife.\n",
      "  APEMANTUS. Where,\n",
      "    For I lay a both best me. Is thine.\n",
      "    Is seet thee, whet ministent where thou antagion,\n",
      "    He's a head thy taught, and as thy brother,\n",
      "    In thy thereon't grow, thy spoke, may, when slain sweet on eart is suppose\n",
      "    him; thy answer hath besides more physics age handes of her saffes are being traffic'd,\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "# context = \"**\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
